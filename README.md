# RAG Application Documentation

## Overview

This application allows users to upload PDF documents, extract the text from them, and query the extracted data using a conversational-style interface. It leverages state-of-the-art technologies like **SentenceTransformer** for embedding-based similarity search, **qDrant** as the vector database, and **Streamlit** for the frontend, with **FastAPI** for the backend.

### Running the application

```bash
docker compose up --build
```

To create a new API key you can login to the app directly and a new key will be created. Else you can use this key directly
"nW87klK-lAD3oaAzsCxfrxs8pc4tG6nmqdQGMgPBZJM="

[You must remember the password for your email, as only one email can be used for an API-KEY]

> If you want to ask questions directly [Client] -> http://localhost:8501/

#### To upload the questions and view the Swagger Docs

> http://localhost:8501/docs

```
Demo pdf is in the root directory of the project.
```

## Architecture

The architecture of the application follows a simple yet scalable design that separates the concerns of the frontend, backend, and the vector database. Here's a high-level overview of the architecture:

1. **Frontend**: Built using **Streamlit**, the frontend provides an easy-to-use interface where users can upload PDF documents and query the extracted text.
2. **Backend**: **FastAPI** serves as the backend, handling file uploads, document parsing, and querying the vector database. It also provides API endpoints for querying the PDF data and interacting with the model.
3. **Text Extraction**: **PyMuPDF** is used to extract text from the uploaded PDF documents.
4. **Model**: **SentenceTransformer** (with the `multi-qa-MiniLM-L6-cos-v1` model) is used to embed the extracted text and queries into vector space for similarity search.
5. **Vector Database**: **qDrant** is used to store the document embeddings (vectors) and perform efficient vector-based search queries.
6. **Authentication**: API key-based authentication is implemented to secure the backend API endpoints.

### Why using the current **Model**?

ðŸ§  Detailed Differences

1. **all-MiniLM-L6-v2**

   Architecture: MiniLM (6-layer)

   Optimized for: Fast semantic similarity

   Best for: Small CPU-based apps, limited memory, or simple search

   Pros: Very lightweight, fast inference

   Cons: Lower embedding quality on complex domains

2. **all-mpnet-base-v2**

   Architecture: MPNet (base)

   Optimized for: High-quality general-purpose embeddings

   Best for: Document chunking + retrieval, FAQ search, summarization base

   Pros: Best overall performance across tasks (STS benchmark)

   Cons: Larger size, slightly slower than MiniLM

3. **multi-qa-mpnet-base-dot-v1**

   Architecture: MPNet (base)

   Fine-tuned for: Multi-lingual QA retrieval

   Similarity Function: Uses dot-product instead of cosine

   Pros: Works well when retrieving full answers to natural questions

   Cons: Requires careful chunking and is sensitive to dot-product scale (sometimes less robust unless training conditions are matched)

## Tools & Technologies

### 1. **SentenceTransformer** (`multi-qa-MiniLM-L6-cos-v1`):

- **Purpose**: This model is used to convert the text from the PDFs and the user queries into dense vector representations (embeddings) that can be used for semantic similarity searches.
- **Reason for choice**: The `multi-qa-MiniLM-L6-cos-v1` model provides a great balance of performance and speed while generating high-quality embeddings suitable for QA systems. It is lightweight, fast, and optimized for sentence-level embeddings, making it perfect for this application.

### 2. **qDrant**:

- **Purpose**: qDrant is used as the vector database to store and search the embeddings generated by the `SentenceTransformer` model.
- **Reason for choice**: qDrant is an efficient and easy-to-use vector database that supports high-performance similarity search. It integrates well with Python and offers features like filtering, multi-dimensional search, and real-time updates, which makes it ideal for this use case.

### 3. **Streamlit** (Frontend):

- **Purpose**: Streamlit is used to build the interactive user interface where users can upload PDF files, view the status of the processing, and ask questions about the extracted data.
- **Reason for choice**: Streamlit allows rapid development of web applications with minimal effort. It offers simple integration with Python code and enables users to interact with the model easily through a clean, intuitive UI.

### 4. **FastAPI** (Backend):

- **Purpose**: FastAPI serves as the backend for handling the file upload, text extraction, model inference, and database interaction.
- **Reason for choice**: FastAPI is known for its high performance, ease of use, and automatic API documentation. It is ideal for building APIs quickly and efficiently, especially for machine learning and data-driven applications like this one.

### 5. **PyMuPDF** (PDF Text Extraction):

- **Purpose**: PyMuPDF is used to extract the text content from the uploaded PDF files.
- **Reason for choice**: PyMuPDF is fast and lightweight. It provides efficient text extraction capabilities, and its integration with Python is simple, making it a great choice for handling PDF files in this application.

### 6. **Docker**:

- **Purpose**: Everything is dockerized for easy deployment and portability.
- **Reason for choice**: Docker ensures that the application runs consistently across different environments (development, testing, and production) by encapsulating the app and all its dependencies in containers. It simplifies the deployment and scaling process.

## Application Flow

1. **File Upload**:
   - The user uploads a PDF document through the Streamlit interface.
   - The backend (FastAPI) processes the uploaded PDF and extracts text using **PyMuPDF**.
2. **Text Embedding**:
   - The extracted text from the PDF is then passed through the **SentenceTransformer** model to generate embeddings.
3. **Data Storage**:
   - The embeddings (vectors) are stored in the **qDrant** vector database for efficient searching.
4. **Query**:
   - The user can query the data via the Streamlit frontend.
   - The query is converted into embeddings using the same **SentenceTransformer** model.
   - The backend uses qDrant to perform a vector-based search to find the most relevant document sections based on the query.
5. **Results Display**:
   - The backend returns the search results to the frontend, where they are displayed to the user.

## Authentication

API key-based authentication is implemented to secure access to the backend API. The API key must be passed in the request headers for accessing endpoints like uploading files and querying the data.

Examples

### Upload file to the api

```bash
curl -X 'POST' \
  'http://localhost:8000/api/pdf/upload' \
  -H 'accept: application/json' \
  -H 'X-API-Key: your-api-key' \
  -H 'Content-Type: multipart/form-data' \
  -F 'file=@Data Structure and Algorithm Notes by Bhpendra Saud.pdf;type=application/pdf'
```

#### Response

> {
> "status": 200,
> "message": "Uploaded successfully"
> }

### Query from pdf

> query = Who is the author?

```bash
curl -X 'GET' \
  'http://localhost:8000/api/pdf/ask?question=Who%20is%20the%20author%3F' \
  -H 'accept: application/json' \
  -H 'X-API-Key: nW87klK-lAD3oaAzsCxfrxs8pc4tG6nmqdQGMgPBZJM='
```

#### Response

> "By Bhupendra Saud"

#

# Future Improvements Checklist

As the application evolves, several improvements can be made to enhance its functionality, performance, and scalability:

- [ ] **Offload PDF embedding to worker threads**:

  - Currently, the app blocks the min thread. Since this is only for a small document of 10 pages, it works fine. Future improvements could include adding celery and asyncio to offload the pdf embedding task to a worker thread which would make it faster without blocking the main thread.

- [ ] **Support for Multiple Document Formats**:

  - Currently, the app only supports PDF uploads. Future improvements could include support for other document formats such as Word, PowerPoint, or HTML. This would require additional text extraction modules or libraries.

- [ ] **Advanced Response Ranking**:

  - Introduce more advanced techniques for ranking search results based on context or user feedback. For example, fine-tuning the model on specific datasets could improve the accuracy of results.

- [ ] **Distributed Search**:

  - For scalability, particularly with large datasets, implement a distributed search system using qDrant's clustering capabilities or sharded database models to scale horizontally.

- [ ] **User Profile and Personalization**:

  - Add user profiles to allow personalization of search results. This could involve storing user preferences and behavior over time to optimize query results based on individual usage patterns.

- [ ] **Multi-language Support**:

  - Support for multiple languages could be introduced by using models trained on non-English data or offering translation services to make the tool accessible to a wider audience.

- [ ] **Caching for Faster Queries**:

  - Implement caching mechanisms for repeated queries to improve response time and reduce the load on the backend. This could involve using Redis or a similar caching service to store frequently accessed results.

- [ ] **Security Enhancements**:

  - Implement role-based access control (RBAC) or OAuth for more secure authentication. The API key authentication could be replaced with a more secure authentication mechanism, especially for production environments.

- [ ] **Logging and Monitoring**:
  - Enhance logging and monitoring capabilities using tools like Prometheus, Grafana, or ELK stack (Elasticsearch, Logstash, Kibana) to track application performance, errors, and user interactions in real-time.
